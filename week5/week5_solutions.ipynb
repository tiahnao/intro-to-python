{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETUP CODE - PlEASE RUN THIS ONCE WHEN YOU STARTUP YOUR CODESPACE\n",
    "\n",
    "# RUN FILE\n",
    "%run 'test/week5_test.ipynb'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 5 - Basic Data Analytics & Data Visualisation\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook is designed to guide you through the fundamentals of basic data analytics and visualization, covering topics such as descriptive statistics, inferential statistics, exploratory data analysis (EDA) including pandas profiling, and data visualization using Matplotlib.\n",
    "\n",
    "By the end of this notebook, you should have a foundational understanding of:\n",
    "\n",
    "- Descriptive statistics and their application.\n",
    "- Conducting correlation tests to explore relationships between variables.\n",
    "- Exploring data distributions and detecting outliers.\n",
    "- Generating insightful EDA reports using `pandas-profiling`.\n",
    "- Utilizing Matplotlib for data visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries needed for this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install ydata_profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "from ydata_profiling import ProfileReport"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descriptive Statistics\n",
    "\n",
    "In this section, we'll cover basic descriptive statistics, including mean, median, mode, and measures of dispersion. Descriptive statistics help us summarize and understand the main features of a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame for Step 1 - Descriptive Statistics\n",
    "data_descriptive = pd.DataFrame({'Values': [5, 8, 2, 7, 1, 6, 9, 2]})\n",
    "print(data_descriptive, \"\\n\")\n",
    "\n",
    "\n",
    "# Calculating mean, median, mode\n",
    "mean_value = data_descriptive['Values'].mean()\n",
    "median_value = data_descriptive['Values'].median()\n",
    "mode_value = data_descriptive['Values'].mode().iloc[0] \n",
    "#Note: if there are multiple values for mode, a dataframe of results would be returned unless .iloc[0] included\n",
    "\n",
    "print(f\"Mean: {mean_value}, Median: {median_value}, Mode: {mode_value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Tests\n",
    "\n",
    "Correlation analysis helps us understand the relationship between two variables.We'll explore correlation tests such as the Pearson correlation test and Spearman rank correlation test, using the scipy stats library. \n",
    "\n",
    "Important things to know about correlation coefficients:\n",
    "- Pearson's correlation is appropriate for linear relationships and continuous variables.\n",
    "- Spearman's correlation is suitable for monotonic relationships and ordinal or continuous variables.\n",
    "- Both coefficients range from -1 to 1, indicating the strength and direction of the relationship.\n",
    "\n",
    "Understanding these correlation coefficients is essential for choosing the appropriate method based on the nature of your data and the relationship you are investigating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data\n",
    "data_corr_x = pd.Series([1, 2, 3, 4, 5])\n",
    "data_corr_y = pd.Series([2, 3, 4, 5, 6])\n",
    "\n",
    "# Pearson correlation\n",
    "pearson_corr, _ = stats.pearsonr(data_corr_x, data_corr_y)\n",
    "\n",
    "# Spearman rank correlation\n",
    "spearman_corr, _ = stats.spearmanr(data_corr_x, data_corr_y)\n",
    "\n",
    "print(f\"Pearson Correlation: {pearson_corr}, Spearman Correlation: {spearman_corr}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Data Distributions\n",
    "\n",
    "Understanding data distributions is crucial for gaining insights into the central tendencies and shapes of datasets.Let's explore descriptive statistics like mean, median, skewness, and kurtosis. \n",
    "\n",
    "- Skewness measures the asymmetry of a distribution. It indicates whether the data is skewed to the left (negatively skewed) or to the right (positively skewed).\n",
    "  - Negative Skewness: The left tail is longer, and the distribution is stretched to the left.\n",
    "  - Positive Skewness: The right tail is longer, and the distribution is stretched to the right.\n",
    "  - Skewness close to 0: The distribution is approximately symmetrical.\n",
    "- Kurtosis measures the tailedness or sharpness of a distribution. It indicates whether the data has heavy or light tails compared to a normal distribution.\n",
    "  - Leptokurtic (Kurtosis > 3): Heavy-tailed distribution with more values in the tails than a normal distribution.\n",
    "  - Mesokurtic (Kurtosis = 3): Similar tailedness to a normal distribution.\n",
    "  - Platykurtic (Kurtosis < 3): Light-tailed distribution with fewer values in the tails than a normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data for EDA\n",
    "data_eda = pd.DataFrame({\n",
    "    'Feature_A': np.random.normal(0, 1, 100),\n",
    "    'Feature_B': np.random.uniform(0, 1, 100)\n",
    "})\n",
    "data_eda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive statistics for feature A column\n",
    "mean_feature_a = data_eda['Feature_A'].mean()\n",
    "median_feature_a = data_eda['Feature_A'].median()\n",
    "skewness_feature_a = data_eda['Feature_A'].skew()\n",
    "kurtosis_feature_a = data_eda['Feature_A'].kurt()\n",
    "\n",
    "# Descriptive statistics for feature B column\n",
    "mean_feature_b = data_eda['Feature_B'].mean()\n",
    "median_feature_b = data_eda['Feature_B'].median()\n",
    "skewness_feature_b = data_eda['Feature_B'].skew()\n",
    "kurtosis_feature_b = data_eda['Feature_B'].kurt()\n",
    "\n",
    "print(f\"FEATURE A \\nMean: {mean_feature_a} \\nMedian: {median_feature_a} \\nSkewness: {skewness_feature_a} \\nKurtosis: {kurtosis_feature_a}\")\n",
    "print(f\"\\nFEATURE B \\nMean: {mean_feature_b} \\nMedian: {median_feature_b} \\nSkewness: {skewness_feature_b} \\nKurtosis: {kurtosis_feature_b}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation Analysis\n",
    "Explore Pearson correlation and visualize correlations using seaborn heatmaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix\n",
    "corr_matrix = data_eda.corr()\n",
    "\n",
    "# Heatmap\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', linewidths=.5)\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlier Detection\n",
    "\n",
    "Outlier detection is essential for understanding anomalies that may impact statistical analyses. Identify and handle outliers using visualization techniques. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset with outliers\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "# Generate a dataset with a normal distribution\n",
    "normal_data = np.random.normal(loc=50, scale=10, size=100)\n",
    "# Add outliers\n",
    "outliers = np.array([20, 80, 90, 110])\n",
    "# Combine the normal data with outliers\n",
    "data_with_outliers = np.concatenate([normal_data, outliers])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plot for outlier detection\n",
    "sns.boxplot(data=data_with_outliers)\n",
    "plt.title('Boxplot for Outlier Detection')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Distribution Exploration\n",
    "\n",
    "Exploring data distributions helps us understand the shape and patterns within the data. Perform univariate and bivariate analysis using histograms, kernel density plots, scatter plots, and pair plots. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Univariate analysis: Histogram\n",
    "sns.histplot(data_eda['Feature_A'], kde=True, color='skyblue', bins=20)\n",
    "plt.title('Histogram for Feature_A')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bivariate analysis: Pair plot\n",
    "g = sns.pairplot(data_eda)\n",
    "g.fig.suptitle(\"Pair Plot for Data Exploration\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas Profiling\n",
    "\n",
    "pandas-profiling is a powerful library for generating profile reports from a Pandas DataFrame. It provides an overview of the dataset, including descriptive statistics, correlations, missing values, and more. It has recently had a change of library name to **ydata_profiling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 100 row dataset made up of 4 numerical columns & 2 string columns\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "# Generate numerical columns\n",
    "num_columns = {\n",
    "    'Numeric1': np.random.randint(1, 100, size=100),\n",
    "    'Numeric2': np.random.uniform(0, 1, size=100),\n",
    "    'Numeric3': np.random.normal(50, 10, size=100),\n",
    "    'Numeric4': np.random.choice([0, 1], size=100),\n",
    "}\n",
    "# Generate string columns\n",
    "str_columns = {\n",
    "    'String1': np.random.choice(['A', 'B', 'C', 'D'], size=100),\n",
    "    'String2': np.random.choice(['X', 'Y', 'Z'], size=100),\n",
    "}\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame({**num_columns, **str_columns})\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile = ProfileReport(df, title=\"Profiling Report\")\n",
    "profile.to_notebook_iframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualization with Matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Plotting\n",
    "\n",
    "Basic plotting is fundamental for visualizing data trends and relationships. Explore basic plotting using Matplotlib for line plots, scatter plots, and bar plots. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data\n",
    "x_values = np.linspace(0, 10, 100)\n",
    "y_values_line = np.sin(x_values)\n",
    "x_values_scatter = np.random.rand(30)\n",
    "y_values_scatter = np.random.rand(30)\n",
    "categories = ['Category A', 'Category B', 'Category C']\n",
    "values_bar = [25, 50, 30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Line plot\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(x_values, y_values_line, label='sin(x)')\n",
    "plt.title('Line Plot')\n",
    "plt.xlabel('X-axis')\n",
    "plt.ylabel('Y-axis')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot\n",
    "plt.scatter(x_values_scatter, y_values_scatter, color='red', marker='o', label='Random Points')\n",
    "plt.title('Scatter Plot')\n",
    "plt.xlabel('X-axis')\n",
    "plt.ylabel('Y-axis')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar plot\n",
    "plt.bar(categories, values_bar, color='green')\n",
    "plt.title('Bar Plot')\n",
    "plt.xlabel('Categories')\n",
    "plt.ylabel('Values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Plots\n",
    "\n",
    "Create subplots and configure figures to combine different types of plots. Multiple plots help us present various aspects of the data in a single visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subplots\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(8, 3))\n",
    "\n",
    "# Scatter plot\n",
    "axes[0].scatter(data_eda['Feature_A'], data_eda['Feature_B'])\n",
    "axes[0].set_title('Scatter Plot for Feature_A vs Feature_B')\n",
    "\n",
    "# Bar plot\n",
    "data_bar = pd.DataFrame({'Category': ['A', 'B', 'C'], 'Values': [10, 15, 5]})\n",
    "axes[1].bar(data_bar['Category'], data_bar['Values'])\n",
    "axes[1].set_title('Bar Plot for Categorical Data')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced Plot Types\n",
    "\n",
    "Advanced plot types provide additional insights into the distribution and spread of the data. Explore advanced plot types like histograms, box plots, and violin plots, using matplotlib & seaborn\n",
    "\n",
    "- Histograms: Good for understanding the overall distribution and frequency of data values.\n",
    "- Box Plots: Useful for summarizing the central tendency and spread of data, as well as identifying outliers.\n",
    "- Violin Plots: Provide a combination of box plot and density plot information, offering insights into distribution shape and density."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data\n",
    "data_histogram = np.random.normal(0, 1, 1000)  # Normally distributed data\n",
    "data_box_violin = [np.random.normal(0, 1, 100), np.random.normal(0, 2, 100), np.random.normal(0, 3, 100)]\n",
    "\n",
    "# Histogram\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.hist(data_histogram, bins=30, color='skyblue', edgecolor='black')\n",
    "plt.title('Histogram')\n",
    "plt.xlabel('Values')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Box plot\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.boxplot(data_box_violin, labels=['Group 1', 'Group 2', 'Group 3'])\n",
    "plt.title('Box Plot')\n",
    "plt.xlabel('Groups')\n",
    "plt.ylabel('Values')\n",
    "\n",
    "# Violin plot\n",
    "plt.subplot(1, 3, 3)\n",
    "sns.violinplot(data=data_box_violin, palette='pastel')\n",
    "plt.title('Violin Plot')\n",
    "plt.xlabel('Groups')\n",
    "plt.ylabel('Values')\n",
    "\n",
    "# Adjust layout and show the plots\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spatial plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "pip install geopandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a geodataframe from a local geodatabase \n",
    "gdf = gpd.read_file(\"/workspaces/intro-to-python/week5/data.gdb\")\n",
    "gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all the methods of the geometry object in geopandas\n",
    "dir(gdf.geometry[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect brisbane subset\n",
    "brisbane_PDAs = (gdf[gdf[\"LGA_NAME\"] == \"Brisbane\"])[['LGA_NAME','PDA_NAME', 'PDA_STATUS', 'REGULATION_CODE', 'geometry']]\n",
    "brisbane_PDAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids_x = []\n",
    "centroids_y = []\n",
    "for _, row in brisbane_PDAs.iterrows():\n",
    "    centroids_x.append(row.geometry.centroid.x)\n",
    "    centroids_y.append(row.geometry.centroid.y)\n",
    "\n",
    "global_centroid_x = sum(centroids_x)/len(centroids_x)\n",
    "global_centroid_y = sum(centroids_y)/len(centroids_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_centroid_x, global_centroid_y # y is latitude and x is longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "pip install folium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "\n",
    "# Initialise folium map object\n",
    "brisbane_pda_map = folium.Map(location=[global_centroid_y, global_centroid_x], zoom_start=12)  # Set the initial map location and zoom\n",
    "brisbane_pda_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folium.GeoJson(brisbane_PDAs).add_to(brisbane_pda_map)\n",
    "brisbane_pda_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge Task\n",
    "\n",
    "Now it's time to see how much you have learnt from this week and last week, across all the topics: data cleaning, manipulation, analysing & visualising. Below is a synthetic dataset about the electricity consumption of 200 households in the US. Using this dataset, please complete the following:\n",
    "- An EDA to see what state the data is currently in. Use whatever method you would like, but briefly describe (in a markdown cell) everything you have learnt about the dataset, prior to applying any cleaning methods.\n",
    "- Apply data cleaning & manipulation techniques where required\n",
    "- Apply your data analytics skills to find out useful information about the cleaned dataset, as well as any relationships between the attributes. Describe your findings in a markdown cell.\n",
    "- Create at least 2 visualisations using the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below is the first 5 rows of the dataset you will use for the Challenge Task. Make sure you have run the first command of this notebook for this to work\n",
    "\n",
    "challenge_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete first EDA here\n",
    "\n",
    "profile = ProfileReport(challenge_df, title=\"Profiling Report\")\n",
    "profile.to_notebook_iframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "challenge_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "challenge_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FINDINGS FROM FIRST EDA:\n",
    "\n",
    "- Whole DF --> 200 rows x 7 columns, no duplicates\n",
    "- Household ID --> no missing values, all unique\n",
    "- Electricity_Consumption_kWh --> 1 missing value, all unique, possible outlier = 400\n",
    "- Region --> 4 areas, N E S W, no missing\n",
    "- Appliace_count --> values 1 to 9, no missing\n",
    "- Monthly_Bill_USD --> 30 missing values, outlier = 500 (frequency = 20 times)\n",
    "- Household_Members --> 1 to 5, no missing values\n",
    "- Has_Solar_Panels --> Boolean = True or False, no missing values\n",
    "\n",
    "DATA CLEANING REQUIRED:\n",
    "\n",
    "- address missing values\n",
    "- investigate & address outliers in Electricity_Consumption_kWh & Monthly_Bill_USD columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete data cleaning / manipulation here\n",
    "\n",
    "challenge_df[challenge_df.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you could not accurately infer the electricity consumption or billing information for this customers, for analysis purposes we will drop these rows from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "challenge_df_cleaned = challenge_df.dropna()\n",
    "challenge_df_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "challenge_df_cleaned[challenge_df_cleaned['Electricity_Consumption_kWh'] > 399]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "challenge_df_cleaned[challenge_df_cleaned['Monthly_Bill_USD'] > 499]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whilst Household_ID 81 has an Electricity_Consumption_kWh outlier reading of 400, it is plausible that this is a correct reading as it only occured once. By contrast, there were 20 customers (Household_ID 151 through to 170) who had an outlier Monthly_Bill_USD of 500, which is over $200 more than the customer with the next highest bill. As this is an anomoly, these rows will be dropped from the dataset, for the purpose of analysis/visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "challenge_df_cleaned = challenge_df_cleaned[challenge_df_cleaned['Monthly_Bill_USD'] < 499]\n",
    "challenge_df_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete second EDA here\n",
    "\n",
    "profile = ProfileReport(challenge_df_cleaned, title=\"Profiling Report\")\n",
    "profile.to_notebook_iframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Findings of second EDA: Dataset cleaned & ready for visualisations.\n",
    "NOTE: You may have taken a different approach to data cleaning, manipulation, analysis & visualisation. This is just one way you could approach the task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete visualisation 1 here\n",
    "\n",
    "# Scatter plot\n",
    "plt.scatter(challenge_df_cleaned['Electricity_Consumption_kWh'], challenge_df_cleaned['Monthly_Bill_USD'], color='red', marker='o', label = 'Customer')\n",
    "plt.title('Scatter Plot of Electricity Consumption and Monthly Bill')\n",
    "plt.xlabel('Electricity_Consumption_kWh')\n",
    "plt.ylabel('Monthly_Bill_USD')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From visualisation 1, it appears that there isn't really a correlation between electricity consumption & monthly bill (for this synthetic dataset). Lets investigate other correlations (or possibly, lack of correlations), using a correlation matrix below for only the numerical attributes (colums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete visualisation 2 here\n",
    "\n",
    "# Correlation matrix\n",
    "corr_matrix = challenge_df_cleaned[['Electricity_Consumption_kWh', 'Appliance_Count', 'Monthly_Bill_USD', 'Household_Members']].corr()\n",
    "\n",
    "# Heatmap\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', linewidths=.5)\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that for this synthetic dataset, there doesn't seem to be a correlation between any of the numerical columns (Electricity consumption, appliance count, monthly bill, or household members), as each of the (absolute) correlation values are close to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

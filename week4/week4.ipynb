{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETUP CODE - PlEASE RUN THIS ONCE WHEN YOU STARTUP YOUR CODESPACE\n",
    "\n",
    "# RUN TEST FILE\n",
    "%run 'test/week4_test.ipynb'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 4 - Data Cleaning and Data Manipulation\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this notebook, we will explore common techniques for data cleaning and manipulation using the Pandas library in Python. Proper data preparation is crucial for accurate analysis and modeling. </br>\n",
    "\n",
    "Below is a data cleaning checklist which contains some useful tips on what to keep an eye out for. </br>\n",
    "\n",
    "<img src = https://images.datacamp.com/image/upload/v1654855433/Data_Cleaning_Checklist_1x_fad5f3e982.png width = \"900\" height = \"2500\" >\n",
    "\n",
    "We are now going to look at how to implement some of these techniques in code. Some other data cleaning techniques will be touched on in next week's notebook.\n",
    "\n",
    "### Import Libraries\n",
    "\n",
    "Let's start by importing the necessary libraries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Missing Values\n",
    "Handling missing values is a critical step in the data cleaning process. Let's create a sample DataFrame with missing values to demonstrate the techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Eva'],\n",
    "        'Age': [25, 30, None, 22, 35],\n",
    "        'Salary': [50000, 60000, 75000, None, 80000]}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for Missing Values </br></br>\n",
    "We can use the isnull() function to identify missing values in the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "missing_vals = df.isnull()\n",
    "print(missing_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "missing_vals_total = df.isnull().sum()\n",
    "print(missing_vals_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill Missing Values\n",
    "\n",
    "Filling missing values is a common strategy. Situational logical should be applied to choose how best to fill in the missing values, and it also depends on the datatype of the column. \n",
    "- If you're looking at a numerical column, you may like to use the mean value, maximum value or minimum value of that column, or alternatively fill the missing values with a 0.\n",
    "- If it is a string column, you may like to fill it with the string 'missing', depending on your use case and/or personal preference.\n",
    "\n",
    "In this example, as we are looking at missing values in numerical columns, we will firstly fill the missing values with a specific value (0), then we will show you how to fill the missing values with the mean of each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values with 0\n",
    "df['Age'].fillna(0, inplace=True)\n",
    "df['Salary'].fillna(0, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values with mean\n",
    "df['Age'].fillna(df['Age'].mean(), inplace=True)\n",
    "df['Salary'].fillna(df['Salary'].mean(), inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did you notice how no values changed to the mean (remained as 0)? This is because when we filled the missing values with 0, we used the \"inplace\" attribute which essentially means the dataframe 'df' was overridden and those values were now saved in place of the missing values, resulting in 'df' not having any missing values. \n",
    "\n",
    "Be mindful when using the inplace attribute, because if you use it in error, you may need to re-run your previous code to get back to the dataframe format/contents you wish to use. An alternative in some cases would be to create a new dataframe.\n",
    "\n",
    "In our case, it would be better to use the column means to fill in the missing values (instead of 0), and we will save the new dataframe as a different dataframe variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values with mean\n",
    "df2 = df.copy()\n",
    "df2['Age'].fillna(df2['Age'].mean(), inplace=True)\n",
    "df2['Salary'].fillna(df2['Salary'].mean(), inplace=True)\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternative to filling missing or null values is to remove the whole row of data. As we didn't override the original dataframe 'df', we will now remove the rows which contain missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The documentation for this function can be found in the link below, which contains explanations of it's various parameters. A common parameter to use is the 'subset' argument, which will only drop a row if there is a missing value in the specified column, and will keep rows which have missing values in other columns\n",
    "\n",
    "https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.dropna.html\n",
    "\n",
    "This is how you would drop rows with missing values in just the salary column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset=['Salary'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Duplicates\n",
    "Duplicate records can skew analysis results. Let's create a DataFrame with duplicates and demonstrate how to handle them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_duplicates = {'ID': [1, 2, 3, 4, 1],\n",
    "                   'Product': ['A', 'B', 'C', 'D', 'A'],\n",
    "                   'Quantity': [10, 20, 15, 30, 10]}\n",
    "\n",
    "df_duplicates = pd.DataFrame(data_duplicates)\n",
    "df_duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check and Remove Duplicates.\n",
    "\n",
    "Checking and removing duplicates is essential for maintaining data integrity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates\n",
    "print(\"Duplicates before removal: \\n\",df_duplicates.duplicated())\n",
    "\n",
    "# Remove duplicates\n",
    "df_duplicates.drop_duplicates(inplace=True)\n",
    "print(\"\\nDataFrame after removing duplicates: \\n\", df_duplicates)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge task 1: Data cleaning example\n",
    "\n",
    "You have been given some fake data below that contains information about employees at a random company. Your task is to perform the following data cleaning steps:\n",
    "\n",
    "- Identify and handle missing values in the dataset. Think about the best way to do this\n",
    "- Remove any duplicate records from the dataset.\n",
    "- Display the cleaned DataFrame.\n",
    "\n",
    "The goal is to apply the data cleaning techniques learned in the notebook to ensure the dataset is ready for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First 5 rows of the dataframe for challenge 1. Make sure you have run the first command of this notebook for this to work\n",
    "\n",
    "challenge_1_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill out your answer below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Manipulation\n",
    "\n",
    "There are various ways you can manipulate data, including selecting specific rows & columns, filtering, grouping, aggregating, pivotting, sorting, changing data types, & joining dataframes together. All of these techniques were covered in the week 3 material, except for the last 3, which will be covered now.\n",
    "\n",
    "### Sorting data\n",
    "\n",
    "Sorting data is crucial for better visualization and analysis. Let's demonstrate how to sort a DataFrame, using df2 (the original dataframe with the missing values filled in by the column mean)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by single column\n",
    "# Sort DataFrame by 'Age' in descending order\n",
    "df2_sorted_age = df2.sort_values(by='Age', ascending=False)\n",
    "df2_sorted_age\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by multiple columns - but first, add in some extra data!\n",
    "new_data = {'Name': ['Frank', 'George', 'Hayley'],\n",
    "            'Age': [25, 30, 22],\n",
    "            'Salary': [60000, 75000, 65000]}\n",
    "new_df = pd.DataFrame(new_data)\n",
    "\n",
    "df2_new = pd.concat([df2, new_df], ignore_index = True) \n",
    "df2_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort DataFrame by 'Age' in ascending order and then by 'Salary' in descending order\n",
    "df2_sorted_multiple = df2_new.sort_values(by=['Age', 'Salary'], ascending=[True, False])\n",
    "df2_sorted_multiple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing Data Types\n",
    "\n",
    "Changing data types is important when the default types assigned by Pandas may not be suitable for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_sorted_multiple.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the data type of the 'Age' column to float\n",
    "df2_sorted_multiple['Age'] = df2_sorted_multiple['Age'].astype(int)\n",
    "print(df2_sorted_multiple.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data types of multiple columns\n",
    "df2_new = df2_new.astype({'Age': 'int', 'Salary': 'float'}) #note, salary was already float (this is just an example)\n",
    "print(df2_new.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joining Datasets\n",
    "\n",
    "Joining datasets is a common operation when combining information from different sources. It is important to keep in mind which type of join you are doing (for example: inner, outer, left, right). Below is a visualisation of a few different types of joins\n",
    "\n",
    "<img src = https://miro.medium.com/v2/resize:fit:900/1*yb76Gk03pZsjVDp79n2yKA.jpeg width = \"400\" height = \"250\" >\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two sample DataFrames\n",
    "df1_join_eg = pd.DataFrame({'ID': [1, 2, 3], 'Name': ['Alice', 'Bob', 'Charlie']})\n",
    "df1_join_eg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_join_eg = pd.DataFrame({'ID': [2, 3, 4], 'Salary': [60000, 75000, 90000]})\n",
    "df2_join_eg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge DataFrames based on 'ID', using 'inner' method\n",
    "merged_df_inner = pd.merge(df1_join_eg, df2_join_eg, on='ID', how='inner')\n",
    "merged_df_inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge DataFrames based on 'ID', using 'outer' method\n",
    "merged_df_outer = pd.merge(df1_join_eg, df2_join_eg, on='ID', how='outer')\n",
    "merged_df_outer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge DataFrames based on 'ID', using 'left' method\n",
    "merged_df_left = pd.merge(df1_join_eg, df2_join_eg, on='ID', how='left')\n",
    "merged_df_left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge DataFrames based on 'ID', using 'right' method\n",
    "merged_df_right= pd.merge(df1_join_eg, df2_join_eg, on='ID', how='right')\n",
    "merged_df_right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also join on multiple attributes (columns). In the above example, you would not merge on multiple columns as both df1_join_eg & df2_join_eg only have 1 column in common. For more information about joining on multiple columns, check out this website: https://sparkbyexamples.com/pandas/pandas-merge-two-dataframes-on-multiple-columns/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge Task 2: Data manipilation example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have two datasets provided below. Your task is to perform the following data manipulation steps:\n",
    "\n",
    "- Apply the following operations on each DataFrame:\n",
    "  1. Select only columns which are relevant to an employee database.\n",
    "  2. Address the missing values in the way you see most logical\n",
    "  3. Join the two DataFrames using both the left join and inner join methods based on the 'ID' column (2 new dataframes will be created).\n",
    "  4. Ensure the datatypes for each column are logically correct.\n",
    "  5. Sort the DataFrame based on the 'Salary' column in descending order.\n",
    "- Display the final result of both joined DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First 5 rows of the dataframe 1 for challenge 2. Make sure you have run the first command of this notebook for this to work\n",
    "\n",
    "challenge_2_df_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First 5 rows of the dataframe 2 for challenge 2. Make sure you have run the first command of this notebook for this to work\n",
    "\n",
    "challenge_2_df_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill out your answer below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
